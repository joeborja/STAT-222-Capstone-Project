{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv('../Data/deathrate_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neJPAcAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "county_name = 'Alameda County, CA'\n",
    "alameda = data[data['County'] == county_name]\n",
    "alameda_y = alameda[alameda[\"Year\"] >= 2000]['Deathrate']\n",
    "alameda_x = alameda[alameda[\"Year\"].isin(range(1999, 2014))][\"Deathrate\"]\n",
    "alameda_X = np.diag(alameda_x)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg = LinearRegression(fit_intercept = False).fit(alameda_X, alameda_y)\n",
    "plt.scatter(range(alameda_y.size), reg.coef_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_counties = 3133\n",
    "all_counties = data[\"County\"].unique()\n",
    "unique_counties = all_counties[:max_counties]\n",
    "\n",
    "y_list = []\n",
    "X_list = []\n",
    "\n",
    "for county_name in unique_counties:\n",
    "    county = data[data[\"County\"] == county_name]\n",
    "    county_y = county[county[\"Year\"].isin(range(2000,2015))][\"Deathrate\"]\n",
    "    county_x = county[county[\"Year\"].isin(range(1999, 2014))][\"Deathrate\"]\n",
    "    county_X = np.diag(county_x)\n",
    "    y_list.append(county_y)\n",
    "    X_list.append(county_X)\n",
    "    \n",
    "y = [element for county_y in y_list for element in county_y ]\n",
    "\n",
    "from scipy.linalg import block_diag\n",
    "\n",
    "X = block_diag(*X_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_ols = y / np.diag(X)\n",
    "beta_ols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of coefficient vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(y)), beta_ols)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of sorted coefficient vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(y)), np.sort(beta_ols))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression solution with prior mean $\\mu = \\underline{1}$.\n",
    "\n",
    "$$\n",
    "\\beta = (X^T X + \\lambda I)^{-1} (X^T y + \\lambda \\underline{1}) =\n",
    "\\begin{pmatrix}\n",
    "\\frac{y_1 x_1 + \\lambda}{x_1^2 + \\lambda}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{y_n x_n + \\lambda}{x_n^2 + \\lambda}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge(y,X, llambda):\n",
    "    x = np.diag(X) \n",
    "    return ((y * x + llambda)/(x**2 + llambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare solutions from OLS to solutions from ridge regression for various settings of $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_comparison(y, X, llambda, sort = False):\n",
    "    reg_ols = ridge(y, X, 0)\n",
    "    \n",
    "    if sort: reg_ols = np.sort(reg_ols)\n",
    "    plt.plot(reg_ols)\n",
    "    \n",
    "    reg_ridge = ridge(y, X, llambda)\n",
    "    \n",
    "    if sort: reg_ridge = np.sort(reg_ridge)\n",
    "    plt.plot(reg_ridge)\n",
    "\n",
    "    is_sorted = \" (sorted)\" if sort == True else \"\"\n",
    "    plt.title(\"Comparison of OLS and Ridge regression coefficient vectors\" + is_sorted )\n",
    "\n",
    "    plt.legend(['OLS', 'Ridge with $\\lambda=$' + str(llambda)])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of methods for Alameda county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison(alameda_y, alameda_X, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of methods for multiple counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison(y,X,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparison(y,X, 100, sort = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression with prior mean $\\mu$ and prior covariance $\\Sigma$.\n",
    "\n",
    "$$\\beta = (X^T X + \\lambda \\Sigma)^{-1} (X^T y + \\lambda \\Sigma \\mu)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge(y, X, llambda, mu = None, Sigma = None):\n",
    "    \n",
    "    x = np.diag(X) \n",
    "    \n",
    "    if mu is None:\n",
    "        mu = np.ones(len(y))\n",
    "        \n",
    "    if Sigma is None:\n",
    "        return ((y*x + llambda * mu) / (x**2 + llambda))\n",
    "    else:\n",
    "        return np.linalg.solve((X.T@X) + (llambda*Sigma), (X.T@y) + llambda*(Sigma@mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: Computing the exact solution may can be slow due to the matrix multiplication and inversion.\n",
    "\n",
    "Question: Can we compute an acceptable approximation in less time using gradient descent?\n",
    "\n",
    "Our loss function is given by\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\|y - X\\beta\\|_2^2 + \\lambda (\\beta-\\underline{1})^T \\Sigma (\\beta - \\underline{1})\n",
    "$$\n",
    "\n",
    "The gradient of this function is\n",
    "\n",
    "$$\n",
    "\\nabla L(\\beta) = 2X^T (X\\beta - y) + 2\\lambda \\Sigma (\\beta - \\underline{1}) = \\begin{pmatrix}\n",
    "2x_1 (x_1 \\beta - y_1) + 2\\lambda (\\beta_1 - 1) \\times \\Sigma_{1,\\cdot} \\\\\n",
    "\\vdots\\\\\n",
    "2x_n(x_n \\beta - y_n) + 2\\lambda(\\beta_1 - 1) \\times \\Sigma_{n,\\cdot}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Gradient descent update:\n",
    "\n",
    "$$\n",
    "\\beta_{t+1} = \\beta_{t} - \\alpha \\nabla L(\\beta_t).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent_update(y, X, alpha, beta, llambda, mu, Sigma):\n",
    "    \n",
    "    x = np.diag(X)\n",
    "    gradient = 2* x**2 * beta - 2 * x * y\n",
    "    \n",
    "    penalty = np.zeros(len(beta))\n",
    "    \n",
    "# Method 1\n",
    "#     for i in range(len(beta)):\n",
    "#         for j in range(len(beta)):\n",
    "#             penalty[i] = penalty[i] + (2*llambda * (beta[j] - mu[j]) * Sigma[i,j])\n",
    "\n",
    "# Method 2\n",
    "#    penalty = [2 * llambda * np.sum([row[j] * (beta[j] - mu[j]) for j in range(len(row))]) for row in Sigma]        \n",
    "\n",
    "# Method 3\n",
    "#    from scipy import sparse\n",
    "#    penalty = 2 * llambda * sparse.csr_matrix(Sigma) @ (beta - mu)\n",
    "\n",
    "# Method 4\n",
    "    penalty = 2 * llambda * Sigma @ (beta - mu)\n",
    "    \n",
    "    gradient += penalty\n",
    "\n",
    "    return np.array(beta - alpha * gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring fastest ways to implement the descent update above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "n = len(y)\n",
    "beta = np.zeros(n)\n",
    "llambda = 10\n",
    "mu = np.ones(n)\n",
    "Sigma = np.eye(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "beta1 = descent_update(y, X, alpha, beta, llambda, mu, Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full gradient descent algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD(input_y, input_X, alpha, llambda, mu, Sigma, max_iter):\n",
    "\n",
    "    beta_initial = np.zeros(len(input_y))\n",
    "    beta = beta_initial\n",
    "    \n",
    "    for t in range(max_iter):\n",
    "        beta = descent_update(input_y, input_X, alpha, beta, llambda, mu, Sigma)\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between gradient descent and the exact solution to get an idea of the step size and number of iterations needed to compute an acceptable approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GD_diagnostic(input_y, input_X, alpha, llambda, mu, Sigma, max_iter):\n",
    "\n",
    "    reg_ridge = ridge(input_y, input_X, llambda)\n",
    "\n",
    "    beta_initial = np.zeros(len(input_y))\n",
    "    beta = beta_initial\n",
    "    \n",
    "    differences = np.zeros(max_iter)\n",
    "    for t in range(max_iter):\n",
    "        beta = descent_update(input_y, input_X, alpha, beta, llambda, mu, Sigma)\n",
    "        differences[t] = np.linalg.norm(reg_ridge - beta)\n",
    "        \n",
    "    plt.plot(range(max_iter), differences)\n",
    "    plt.title(\"Distance from exact solution versus number of iterations\")\n",
    "    plt.show()\n",
    "    print('Distance from exact solution for last iteration is ' + str(differences[-1]))\n",
    "    \n",
    "    plt.plot(range(len(reg_ridge)), reg_ridge)\n",
    "    plt.plot(range(len(reg_ridge)), beta)\n",
    "    plt.title(\"Comparison of Ridge regression coefficient vectors and gradient descent\")\n",
    "    plt.legend(['Ridge with $\\lambda = $' + str(llambda), 'Gradient descent with stepsize = ' + str(alpha)])\n",
    "    # Note: Only a relevant comparison for Sigma = Identity. \n",
    "    # There is not closed form available for comparison when Sigma != Identity.\n",
    "    # We want a sense of how many gradient iterations give us a similar answer to the exact solution.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on single county case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD_diagnostic(alameda_y, alameda_X, alpha = 0.001, llambda = 100, mu = np.ones(15), Sigma = np.eye(15), max_iter = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple counties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GD_diagnostic(y, X, alpha = 0.001, llambda = 100, mu = np.ones(len(y)), Sigma = np.eye(len(y)), max_iter = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent seems to produce an answer roughly equal to the exact solution after 20-30 iterations with step size equal to 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is consistently slower the computing the exact solution for the case $\\mu = (1,\\dots,1)$ and $\\Sigma = I$. For more general $\\Sigma$, we need to compute the normal equations. Fifty iterations of gradient descent produces a reasonable solution much faster than solving the linear system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(y)\n",
    "Z = np.random.randn(n)\n",
    "Sigma = np.eye(n) + np.outer(Z,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "beta = GD(y, X, alpha = 0.001, llambda = 100, mu = np.ones(len(y)), Sigma = Sigma, max_iter = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 100 counties, the solving the linear system takes 55 seconds and gradient descent takes 420ms to reach 50 iterations.\n",
    "\n",
    "For 500 counties, gradient descent takes 20s to reach 50 iterations.\n",
    "\n",
    "For 1000 counties, gradient descent takes 1 min 18s to reach 50 iterations.\n",
    "\n",
    "For all 3313 counties...my computer crashes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
